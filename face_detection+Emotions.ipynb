{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of face_detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNQurF+0lYrFkF6BIBFFo1b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Souguiroumaima7/deep-learning-projects/blob/main/face_detection%2BEmotions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import imutils\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "#import libraries\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import pandas as pd\n",
        "import subprocess\n",
        "import os\n",
        "import time\n",
        "os.environ['WANDB_NOTEBOOK_NAME'] = 'EmotionClassifier'\n",
        "from keras.models import sequential \n",
        "from keras.layers import Dense, Dropout,Flatten \n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Conv2D  \n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "import argparse\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "HYJMJ1iJmSVl"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def take_photo(filename='image2.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename"
      ],
      "metadata": {
        "id": "x3hXooiunHs1"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_file = take_photo()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "YfcGUflAnY17",
        "outputId": "6146d0bd-b2c4-4895-906c-50257deea8ed"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function takePhoto(quality) {\n",
              "      const div = document.createElement('div');\n",
              "      const capture = document.createElement('button');\n",
              "      capture.textContent = 'Capture';\n",
              "      div.appendChild(capture);\n",
              "\n",
              "      const video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
              "\n",
              "      document.body.appendChild(div);\n",
              "      div.appendChild(video);\n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      // Resize the output to fit the video element.\n",
              "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
              "\n",
              "      // Wait for Capture to be clicked.\n",
              "      await new Promise((resolve) => capture.onclick = resolve);\n",
              "\n",
              "      const canvas = document.createElement('canvas');\n",
              "      canvas.width = video.videoWidth;\n",
              "      canvas.height = video.videoHeight;\n",
              "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
              "      stream.getVideoTracks()[0].stop();\n",
              "      div.remove();\n",
              "      return canvas.toDataURL('image/jpeg', quality);\n",
              "    }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#image = cv2.imread(image_file, cv2.IMREAD_UNCHANGED)\n",
        "image = cv2.imread(image_file)\n",
        "\n",
        "# resize it to have a maximum width of 400 pixels\n",
        "image = imutils.resize(image, width=400)\n",
        "(h, w) = image.shape[:2]\n",
        "print(w,h)\n",
        "cv2_imshow(image)"
      ],
      "metadata": {
        "id": "DUaIFovvnhXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -N https://raw.githubusercontent.com/opencv/opencv/master/samples/dnn/face_detector/deploy.prototxt\n",
        "!wget -N https://raw.githubusercontent.com/opencv/opencv_3rdparty/dnn_samples_face_detector_20170830/res10_300x300_ssd_iter_140000.caffemodel"
      ],
      "metadata": {
        "id": "vcrbNa9boChI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"[INFO] loading model...\")\n",
        "prototxt = 'deploy.prototxt'\n",
        "model = 'res10_300x300_ssd_iter_140000.caffemodel'\n",
        "net = cv2.dnn.readNetFromCaffe(prototxt, model)"
      ],
      "metadata": {
        "id": "CMcD8aKooKom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# resize it to have a maximum width of 400 pixels\n",
        "image = imutils.resize(image, width=400)\n",
        "blob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))"
      ],
      "metadata": {
        "id": "A_u_RzLboRmq"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"[INFO] computing object detections...\")\n",
        "net.setInput(blob)\n",
        "detections = net.forward()"
      ],
      "metadata": {
        "id": "CU20q8WroeWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, detections.shape[2]):\n",
        "\n",
        "\t# extract the confidence (i.e., probability) associated with the prediction\n",
        "\tconfidence = detections[0, 0, i, 2]\n",
        "\n",
        "\t# filter out weak detections by ensuring the `confidence` is\n",
        "\t# greater than the minimum confidence threshold\n",
        "\tif confidence > 0.5:\n",
        "\t\t# compute the (x, y)-coordinates of the bounding box for the object\n",
        "\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n",
        "\t\t# draw the bounding box of the face along with the associated probability\n",
        "\t\ttext = \"{:.2f}%\".format(confidence * 100)\n",
        "\t\ty = startY - 10 if startY - 10 > 10 else startY + 10\n",
        "\t\tcv2.rectangle(image, (startX, startY), (endX, endY), (0, 0, 255), 2)\n",
        "\t\tcv2.putText(image, text, (startX, y),\n",
        "\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)"
      ],
      "metadata": {
        "id": "ZBHcB-g3oeTu"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv2_imshow(image)"
      ],
      "metadata": {
        "id": "kbCsI-spopqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}"
      ],
      "metadata": {
        "id": "bVwvrBvblwCr"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage.io import imread\n",
        "data = imread(image_file)"
      ],
      "metadata": {
        "id": "N4Nt7rqt5ux1"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "def findmouth(img):\n",
        "\n",
        "  # INITIALIZE: loading the classifiers\n",
        "  haarFace = cv.Load('haarcascade_frontalface_default.xml')\n",
        "  haarMouth = cv.Load('haarcascade_mouth.xml')\n",
        "  # running the classifiers\n",
        "  storage = cv.CreateMemStorage()\n",
        "  detectedFace = cv.HaarDetectObjects(img, haarFace, storage)\n",
        "  detectedMouth = cv.HaarDetectObjects(img, haarMouth, storage)\n",
        "\n",
        "  # FACE: find the largest detected face as detected face\n",
        "  maxFaceSize = 0\n",
        "  maxFace = 0\n",
        "  if detectedFace:\n",
        "   for face in detectedFace: # face: [0][0]: x; [0][1]: y; [0][2]: width; [0][3]: height \n",
        "    if face[0][3]* face[0][2] > maxFaceSize:\n",
        "      maxFaceSize = face[0][3]* face[0][2]\n",
        "      maxFace = face\n",
        "  \n",
        "  if maxFace == 0: # did not detect face\n",
        "    return 2\n",
        "\n",
        "  def mouth_in_lower_face(mouth,face):\n",
        "    # if the mouth is in the lower 2/5 of the face \n",
        "    # and the lower edge of mouth is above that of the face\n",
        "    # and the horizontal center of the mouth is the center of the face\n",
        "    if (mouth[0][1] > face[0][1] + face[0][3] * 3 / float(5) \n",
        "      and mouth[0][1] + mouth[0][3] < face[0][1] + face[0][3]\n",
        "      and abs((mouth[0][0] + mouth[0][2] / float(2)) \n",
        "        - (face[0][0] + face[0][2] / float(2))) < face[0][2] / float(10)):\n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "\n",
        "  # FILTER MOUTH\n",
        "  filteredMouth = []\n",
        "  if detectedMouth:\n",
        "   for mouth in detectedMouth:\n",
        "    if mouth_in_lower_face(mouth,maxFace):\n",
        "      filteredMouth.append(mouth) \n",
        "  \n",
        "  maxMouthSize = 0\n",
        "  for mouth in filteredMouth:\n",
        "    if mouth[0][3]* mouth[0][2] > maxMouthSize:\n",
        "      maxMouthSize = mouth[0][3]* mouth[0][2]\n",
        "      maxMouth = mouth\n",
        "      \n",
        "  try:\n",
        "    return maxMouth\n",
        "  except UnboundLocalError:\n",
        "    return 2"
      ],
      "metadata": {
        "id": "ErRmlH9O8Mgj"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset():\n",
        "    import csv\n",
        "    with  open(image_file,'r') as f:\n",
        "        data=csv.reader(f)\n",
        "        X=[]\n",
        "        Y=[]\n",
        "        i=1\n",
        "        for row in data:\n",
        "            if i:    #skip first row (header)\n",
        "                i=0\n",
        "                continue    \n",
        "            Y.append(int(row[0]))\n",
        "            X.append([int(n) for n in row[1].split(' ')])\n",
        "    return (np.array(X),np.array(Y))"
      ],
      "metadata": {
        "id": "kvA07pYf_8T1"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "    from tensorflow.keras.models import Model\n",
        "    from tensorflow.keras.layers import Input,Conv2D,Dense,BatchNormalization,Activation\n",
        "    from tensorflow.keras.layers import Dropout,MaxPooling2D,AveragePooling2D,Flatten\n",
        "\n",
        "    inputs=Input(shape=(48,48,1))\n",
        "\n",
        "    X=Conv2D(filters=32,kernel_size=(7,7),padding='same')(inputs)\n",
        "    X=BatchNormalization(axis=-1)(X)\n",
        "    X=Activation('relu')(X)\n",
        "    X=MaxPooling2D((2,2))(X)\n",
        "    X=Dropout(.25)(X)\n",
        "\n",
        "    X=Conv2D(filters=64,kernel_size=(3,3),padding='same')(X)\n",
        "    X=BatchNormalization(axis=-1)(X)\n",
        "    X=Activation('relu')(X)\n",
        "    X=MaxPooling2D((2,2))(X)\n",
        "    X=Dropout(.25)(X)\n",
        "\n",
        "    X=Conv2D(filters=128,kernel_size=(3,3),padding='same')(X)\n",
        "    X=BatchNormalization(axis=-1)(X)\n",
        "    X=Activation('relu')(X)\n",
        "    X=MaxPooling2D((2,2))(X)\n",
        "    X=Dropout(.25)(X)  \n",
        "\n",
        "    X=Conv2D(filters=256,kernel_size=(3,3),padding='same')(X)\n",
        "    X=BatchNormalization(axis=-1)(X)\n",
        "    X=Activation('relu')(X)\n",
        "    X=AveragePooling2D((6,6))(X) \n",
        "    X=Dropout(.25)(X)\n",
        "\n",
        "    X=Flatten()(X)\n",
        "    X=Dense(256)(X)\n",
        "    X=BatchNormalization(axis=-1)(X)\n",
        "    X=Activation('relu')(X)\n",
        "    X=Dropout(.25)(X)\n",
        "\n",
        "    X=Dense(128)(X)\n",
        "    X=BatchNormalization(axis=-1)(X)\n",
        "    X=Activation('relu')(X)\n",
        "    X=Dropout(.25)(X)\n",
        "\n",
        "    X=Dense(7,'softmax')(X)\n",
        "\n",
        "    model=Model(inputs=inputs,outputs=X)\n",
        "    return model"
      ],
      "metadata": {
        "id": "yJCEhKq-Celw"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_pretrained_model=True\n",
        "\n",
        "if load_pretrained_model: \n",
        "    from tensorflow.keras.models import load_model \n",
        "    print('model is loaded successfully')\n",
        "    \n",
        "else:\n",
        "    #create model with 4 conv layers and 2 FC layers\n",
        "    model=create_model() \n",
        "\n",
        "    #compile the model\n",
        "    model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "    #train the model\n",
        "    num_iterations=114   \n",
        "    model.fit(X_train,Y_train_oh,batch_size=64,epochs=num_iterations,validation_split=0.08,verbose=2)\n",
        "    \n",
        "    #save model  \n",
        "    model.save(image,save_format='jpg')\n",
        "    \n",
        "    print('model is trained and saved successfully')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EnGJvllCuHe",
        "outputId": "38f26063-6800-4947-823b-96b752f503a0"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model is loaded successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred=model.predict(X_test)   \n",
        "pred_class=np.argmax(pred,axis=1) \n",
        "index=0     \n",
        "print('Predicted class:',classes_dict[pred_class[index]])\n",
        "print('Actual class: ',classes_dict[Y_test.squeeze()[index]])\n",
        "plt.imshow(X_test[index].squeeze())"
      ],
      "metadata": {
        "id": "xWYg12jSDz6O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}